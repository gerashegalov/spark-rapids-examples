{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to XGBoost Spark with GPU\n",
    "\n",
    "Taxi is an example of XGBoost regressor. This notebook will show you how to load data, train the XGBoost model and use this model to predict \"fare_amount\" of your taxi trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing  [6404](https://github.com/NVIDIA/spark-rapids/issues/6404)\n",
    "\n",
    "Start Jupyter in spark-rapids-examples root\n",
    "```Bash\n",
    "SPARK_HOME=~/dist/spark-3.1.1-bin-hadoop3.2 DATA_ROOT=$PWD/datasets jupyter notebook\n",
    "```\n",
    "\n",
    "## Load libraries\n",
    "First load some common libraries will be used by both GPU version and CPU version XGBoost.\n",
    "\n",
    "\n",
    "There choose exactly one of the following init_spark cells under working/broken.\n",
    "\n",
    "If you want to try another init_spark cell, go to \"Kernel -> Restart & Clear Output\" before proceeding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Working config for rapids-4-spark 21.08 (before shim rework) relying on static extraClassPath\n",
    " This is equivalent to moving jars unders $SPARK_HOME/jars but better because the SPARK_HOME remains immutable\n",
    " \n",
    " But it's still worse than using --jars / --packages, since as far as extraClassPath for executors [\"Users typically should not need to set\n",
    "    this option\"](https://github.com/apache/spark/blob/25759a0de6dd09ecc440d009fba6d661558e7261/docs/configuration.md?plain=1#L561) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "import os\n",
    "\n",
    "m2dir = os.path.expanduser('~/.m2/repository')\n",
    "\n",
    "xgboost_v = '1.6.1'\n",
    "spark_rapids_v = '21.08.0'\n",
    "cudf_v = '21.08.2'\n",
    "\n",
    "launcher.conf.spark.driver.extraClassPath = [\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-gpu_2.12/{xgboost_v}/xgboost4j-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-spark-gpu_2.12/{xgboost_v}/xgboost4j-spark-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/com/nvidia/rapids-4-spark_2.12/{spark_rapids_v}/rapids-4-spark_2.12-{spark_rapids_v}.jar\",\n",
    "    f\"{m2dir}/ai/rapids/cudf/{cudf_v}/cudf-{cudf_v}-cuda11.jar\"\n",
    "]\n",
    "launcher.conf.spark.plugins = \"com.nvidia.spark.SQLPlugin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Broken config for rapids-4-spark 21.08 (before shim rework) relying on --jars"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xgbRegressor.fit will throw \n",
    "\n",
    "java.lang.ClassCastException: ml.dmlc.xgboost4j.scala.Booster cannot be cast to ml.dmlc.xgboost4j.scala.Booster\n",
    "  at ml.dmlc.xgboost4j.scala.spark.XGBoost$.trainDistributed(XGBoost.scala:431)\n",
    "  at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor.train(XGBoostRegressor.scala:190)\n",
    "  at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor.train(XGBoostRegressor.scala:37)\n",
    "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
    "  at $anonfun$model$1(<console>:36)\n",
    "  at Benchmark$.time(<console>:31)\n",
    "  ... 37 elided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "import os\n",
    "\n",
    "m2dir = os.path.expanduser('~/.m2/repository')\n",
    "\n",
    "xgboost_v = '1.6.1'\n",
    "spark_rapids_v = '21.08.0'\n",
    "cudf_v = '21.08.2'\n",
    "\n",
    "launcher.jars = [\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-gpu_2.12/{xgboost_v}/xgboost4j-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-spark-gpu_2.12/{xgboost_v}/xgboost4j-spark-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/com/nvidia/rapids-4-spark_2.12/{spark_rapids_v}/rapids-4-spark_2.12-{spark_rapids_v}.jar\",\n",
    "    f\"{m2dir}/ai/rapids/cudf/{cudf_v}/cudf-{cudf_v}-cuda11.jar\"\n",
    "]\n",
    "launcher.conf.spark.plugins = \"com.nvidia.spark.SQLPlugin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Broken config for rapids-4-spark 21.08 (before shim rework) relying on --packages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xgbRegressor.fit will similarly throw because it's basically the same as --jars just dependencies are downloaded first from Maven\n",
    "\n",
    "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.112.214.106, DMLC_TRACKER_PORT=49291, DMLC_NUM_WORKER=1}\n",
    "java.lang.ClassCastException: ml.dmlc.xgboost4j.scala.Booster cannot be cast to ml.dmlc.xgboost4j.scala.Booster\n",
    "  at ml.dmlc.xgboost4j.scala.spark.XGBoost$.trainDistributed(XGBoost.scala:431)\n",
    "  at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor.train(XGBoostRegressor.scala:190)\n",
    "  at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor.train(XGBoostRegressor.scala:37)\n",
    "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
    "  at $anonfun$model$1(<console>:36)\n",
    "  at Benchmark$.time(<console>:31)\n",
    "  ... 37 elided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "import os\n",
    "\n",
    "m2dir = os.path.expanduser('~/.m2/repository')\n",
    "\n",
    "xgboost_v = '1.6.1'\n",
    "spark_rapids_v = '21.08.0'\n",
    "cudf_v = '21.08.2'\n",
    "\n",
    "launcher.repositories = [\n",
    "    \"https://oss.sonatype.org/content/repositories/releases\"\n",
    "]\n",
    "\n",
    "launcher.packages = [\n",
    "    f\"ml.dmlc:xgboost4j-spark-gpu_2.12:{xgboost_v}\",\n",
    "    f\"com.nvidia:rapids-4-spark_2.12:{spark_rapids_v}\",\n",
    "    f\"ai.rapids:cudf:{cudf_v}\"\n",
    "]\n",
    "launcher.conf.spark.plugins = \"com.nvidia.spark.SQLPlugin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working config for rapids-4-spark 22.08 relying on static extraClassPath and default force.caller.classloader=true\n",
    "\n",
    "it works \"almost by accident\" because XGBoost4j 1.6.1 does not even compile against 21.10+. But force.caller.classloader=true makes it possible for xgboost4j to access GpuColumnVector at runtime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "import os\n",
    "\n",
    "m2dir = os.path.expanduser('~/.m2/repository')\n",
    "\n",
    "xgboost_v = '1.6.1'\n",
    "spark_rapids_v = '22.08.0'\n",
    "\n",
    "\n",
    "launcher.conf.spark.driver.extraClassPath = [\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-gpu_2.12/{xgboost_v}/xgboost4j-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-spark-gpu_2.12/{xgboost_v}/xgboost4j-spark-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/com/nvidia/rapids-4-spark_2.12/{spark_rapids_v}/rapids-4-spark_2.12-{spark_rapids_v}.jar\"\n",
    "]\n",
    "launcher.conf.spark.plugins = \"com.nvidia.spark.SQLPlugin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broken config for rapids-4-spark 22.08 relying on static extraClassPath but changing force.caller.classloader=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "import os\n",
    "\n",
    "m2dir = os.path.expanduser('~/.m2/repository')\n",
    "\n",
    "xgboost_v = '1.6.1'\n",
    "spark_rapids_v = '22.08.0'\n",
    "\n",
    "\n",
    "launcher.conf.spark.driver.extraClassPath = [\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-gpu_2.12/{xgboost_v}/xgboost4j-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-spark-gpu_2.12/{xgboost_v}/xgboost4j-spark-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/com/nvidia/rapids-4-spark_2.12/{spark_rapids_v}/rapids-4-spark_2.12-{spark_rapids_v}.jar\"\n",
    "]\n",
    "launcher.conf.spark.plugins = \"com.nvidia.spark.SQLPlugin\"\n",
    "launcher.conf.set(\"spark.rapids.force.caller.classloader\", False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This reproduces the bug in foreachParttion\n",
    "\n",
    "\n",
    "// start transform\n",
    "val (prediction, _) = Benchmark.time(\"transform\") {\n",
    "  val ret = model.transform(transSet).cache()\n",
    "  ret.foreachPartition((_: Iterator[_]) => ())\n",
    "  ret\n",
    "}\n",
    "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.lang.NoClassDefFoundError: com/nvidia/spark/rapids/GpuColumnVector\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.$anonfun$loadNextBatch$2(GpuPreXGBoost.scala:314)\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$.withResource(GpuPreXGBoost.scala:576)\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.loadNextBatch(GpuPreXGBoost.scala:299)\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.hasNext(GpuPreXGBoost.scala:337)\n",
    "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
    "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
    "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
    "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
    "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n",
    "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
    "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
    "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
    "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
    "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
    "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
    "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
    "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
    "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
    "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
    "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
    "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
    "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
    "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
    "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
    "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
    "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
    "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
    "\tat java.lang.Thread.run(Thread.java:750)\n",
    "Caused by: java.lang.ClassNotFoundException: com.nvidia.spark.rapids.GpuColumnVector\n",
    "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
    "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
    "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n",
    "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
    "\t... 47 more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence, rapids-4-spark 22.10.0-SNAPSHOT implementing force.caller.classloader=false as the only call path is now broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "import os\n",
    "\n",
    "m2dir = os.path.expanduser('~/.m2/repository')\n",
    "\n",
    "xgboost_v = '1.6.1'\n",
    "spark_rapids_v = '22.10.0-SNAPSHOT'\n",
    "launcher.conf.spark.driver.extraClassPath = [\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-gpu_2.12/{xgboost_v}/xgboost4j-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/ml/dmlc/xgboost4j-spark-gpu_2.12/{xgboost_v}/xgboost4j-spark-gpu_2.12-{xgboost_v}.jar\",\n",
    "    f\"{m2dir}/com/nvidia/rapids-4-spark_2.12/{spark_rapids_v}/rapids-4-spark_2.12-{spark_rapids_v}.jar\"\n",
    "]\n",
    "launcher.conf.spark.plugins = \"com.nvidia.spark.SQLPlugin\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "broken the same way as above\n",
    "\n",
    "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.lang.NoClassDefFoundError: com/nvidia/spark/rapids/GpuColumnVector\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.$anonfun$loadNextBatch$2(GpuPreXGBoost.scala:314)\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$.withResource(GpuPreXGBoost.scala:576)\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.loadNextBatch(GpuPreXGBoost.scala:299)\n",
    "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.hasNext(GpuPreXGBoost.scala:337)\n",
    "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1662546994040)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import ml.dmlc.xgboost4j.scala.spark.{XGBoostRegressor, XGBoostRegressionModel}\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ml.dmlc.xgboost4j.scala.spark.{XGBoostRegressor, XGBoostRegressionModel}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides CPU version requires some extra libraries, such as:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.FloatType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataRoot: String = /home/gshegalov/gits/NVIDIA/spark-rapids-examples/datasets\n",
       "trainPath: String = /home/gshegalov/gits/NVIDIA/spark-rapids-examples/datasets/taxi/csv/train/\n",
       "evalPath: String = /home/gshegalov/gits/NVIDIA/spark-rapids-examples/datasets/taxi/csv/test/\n",
       "transPath: String = /home/gshegalov/gits/NVIDIA/spark-rapids-examples/datasets/taxi/csv/test/\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// You need to update them to your real paths! The input data files can be the output of taxi-etl jobs, or you can\n",
    "// just use the provided sample datasets upder datasets path. \n",
    "val dataRoot = sys.env.getOrElse(\"DATA_ROOT\", \"/data\")\n",
    "val trainPath = dataRoot + \"/taxi/csv/train/\"\n",
    "val evalPath  = dataRoot + \"/taxi/csv/test/\"\n",
    "val transPath = dataRoot + \"/taxi/csv/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the schema of the dataset\n",
    "The Taxi data has 16 columns: 15 features and 1 label. \"fare_amount\" is the label column. The schema will be used to load data in the future. \n",
    "\n",
    "The next block also defines some key parameters used in XGBoost training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelName: String = fare_amount\n",
       "schema: org.apache.spark.sql.types.StructType = <lazy>\n",
       "featureNames: Array[String] = Array(vendor_id, passenger_count, trip_distance, pickup_longitude, pickup_latitude, rate_code, store_and_fwd, dropoff_longitude, dropoff_latitude, hour, year, month, day, day_of_week, is_weekend)\n",
       "paramMap: scala.collection.immutable.Map[String,Int] = <lazy>\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelName = \"fare_amount\"\n",
    "lazy val schema =\n",
    "  StructType(Array(\n",
    "    StructField(\"vendor_id\", DoubleType),\n",
    "    StructField(\"passenger_count\", DoubleType),\n",
    "    StructField(\"trip_distance\", DoubleType),\n",
    "    StructField(\"pickup_longitude\", DoubleType),\n",
    "    StructField(\"pickup_latitude\", DoubleType),\n",
    "    StructField(\"rate_code\", DoubleType),\n",
    "    StructField(\"store_and_fwd\", DoubleType),\n",
    "    StructField(\"dropoff_longitude\", DoubleType),\n",
    "    StructField(\"dropoff_latitude\", DoubleType),\n",
    "    StructField(labelName, DoubleType),\n",
    "    StructField(\"hour\", DoubleType),\n",
    "    StructField(\"year\", IntegerType),\n",
    "    StructField(\"month\", IntegerType),\n",
    "    StructField(\"day\", DoubleType),\n",
    "    StructField(\"day_of_week\", DoubleType),\n",
    "    StructField(\"is_weekend\", DoubleType)\n",
    "  ))\n",
    "\n",
    "val featureNames = schema.filter(_.name != labelName).map(_.name).toArray\n",
    "\n",
    "lazy val paramMap = Map(\n",
    "  \"num_round\" -> 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new spark session and load data\n",
    "\n",
    "A new spark session should be created to continue all the following spark operations.\n",
    "\n",
    "NOTE: in this notebook, the dependency jars have been loaded when installing toree kernel. Alternatively the jars can be loaded into notebook by [%AddJar magic](https://toree.incubator.apache.org/docs/current/user/faq/). However, there's one restriction for `%AddJar`: the jar uploaded can only be available when `AddJar` is called just after a new spark session is created. Do it as below:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"taxi-GPU\").getOrCreate\n",
    "%AddJar file:/data/libs/rapids-4-spark-XXX.jar\n",
    "%AddJar file:/data/libs/xgboost4j-spark-gpu_2.12-XXX.jar\n",
    "%AddJar file:/data/libs/xgboost4j-gpu_2.12-XXX.jar\n",
    "// ...\n",
    "```\n",
    "\n",
    "##### Please note the new jar \"rapids-4-spark-XXX.jar\" is only needed for GPU version, you can not add it to dependence list for CPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5e6227e7\n",
       "reader: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@2f6021ea\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Build the spark session and data reader as usual\n",
    "val sparkSession = SparkSession.builder().appName(\"taxi-GPU\").getOrCreate\n",
    "val reader = sparkSession.read.option(\"header\", true).schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSet: org.apache.spark.sql.DataFrame = [vendor_id: double, passenger_count: double ... 14 more fields]\n",
       "evalSet: org.apache.spark.sql.DataFrame = [vendor_id: double, passenger_count: double ... 14 more fields]\n",
       "transSet: org.apache.spark.sql.DataFrame = [vendor_id: double, passenger_count: double ... 14 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Please make sure to change the api to reader.parquet if you load parquet files.\n",
    "val trainSet = reader.csv(trainPath)\n",
    "val evalSet  = reader.csv(evalPath)\n",
    "val transSet = reader.csv(transPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set XGBoost parameters and build a XGBoostRegressor\n",
    "\n",
    "For CPU version, `num_workers` is recommended being equal to the number of CPU cores, while for GPU version, it should be set to the number of GPUs in Spark cluster.\n",
    "\n",
    "Besides the `tree_method` for CPU version is also different from that for GPU version. Now only \"gpu_hist\" is supported for training on GPU.\n",
    "\n",
    "```scala\n",
    "// difference in parameters\n",
    "  \"num_workers\" -> 12\n",
    "  \"tree_method\" -> \"hist\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgbParamFinal: scala.collection.immutable.Map[String,Any] = Map(num_round -> 100, tree_method -> gpu_hist, num_workers -> 1)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xgbParamFinal = paramMap ++ Map(\"tree_method\" -> \"gpu_hist\", \"num_workers\" -> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgbRegressor: ml.dmlc.xgboost4j.scala.spark.XGBoostRegressor = xgbr_df31a9096020\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xgbRegressor = new XGBoostRegressor(xgbParamFinal)\n",
    "  .setLabelCol(labelName)\n",
    "  .setFeaturesCol(featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark and train\n",
    "The object `benchmark` is used to compute the elapsed time of some operations.\n",
    "\n",
    "Training with evaluation sets is also supported in 2 ways, the same as CPU version's behavior:\n",
    "\n",
    "* Call API `setEvalSets` after initializing an XGBoostRegressor\n",
    "\n",
    "```scala\n",
    "xgbRegressor.setEvalSets(Map(\"eval\" -> evalSet))\n",
    "\n",
    "```\n",
    "\n",
    "* Use parameter `eval_sets` when initializing an XGBoostRegressor\n",
    "\n",
    "```scala\n",
    "val paramMapWithEval = paramMap + (\"eval_sets\" -> Map(\"eval\" -> evalSet))\n",
    "val xgbRegressorWithEval = new XGBoostRegressor(paramMapWithEval)\n",
    "```\n",
    "\n",
    "Here chooses the API way to set evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: xgbRegressor.type = xgbr_df31a9096020\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbRegressor.setEvalSets(Map(\"eval\" -> evalSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Benchmark\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object Benchmark {\n",
    "  def time[R](phase: String)(block: => R): (R, Float) = {\n",
    "    val t0 = System.currentTimeMillis\n",
    "    val result = block // call-by-name\n",
    "    val t1 = System.currentTimeMillis\n",
    "    println(\"Elapsed time [\" + phase + \"]: \" + ((t1 - t0).toFloat / 1000) + \"s\")\n",
    "    (result, (t1 - t0).toFloat / 1000)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.112.214.106, DMLC_TRACKER_PORT=39103, DMLC_NUM_WORKER=1}\n",
      "Elapsed time [train]: 5.551s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: ml.dmlc.xgboost4j.scala.spark.XGBoostRegressionModel = xgbr_df31a9096020\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// start training\n",
    "val (model, _) = Benchmark.time(\"train\") {\n",
    "  xgbRegressor.fit(trainSet)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation and evaluation\n",
    "Here uses `transSet` to evaluate our model and use some key columns to show our predictions. Finally we use `RegressionEvaluator` to calculate an overall `rmse` of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.lang.NoClassDefFoundError: com/nvidia/spark/rapids/GpuColumnVector",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.lang.NoClassDefFoundError: com/nvidia/spark/rapids/GpuColumnVector",
      "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.$anonfun$loadNextBatch$2(GpuPreXGBoost.scala:314)",
      "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$.withResource(GpuPreXGBoost.scala:576)",
      "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.loadNextBatch(GpuPreXGBoost.scala:299)",
      "\tat ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.hasNext(GpuPreXGBoost.scala:337)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:750)",
      "Caused by: java.lang.ClassNotFoundException: com.nvidia.spark.rapids.GpuColumnVector",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)",
      "\t... 47 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)",
      "  at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)",
      "  at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:2906)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:3676)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3674)",
      "  at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2906)",
      "  at $anonfun$prediction$1(<console>:37)",
      "  at Benchmark$.time(<console>:31)",
      "  ... 37 elided",
      "Caused by: java.lang.NoClassDefFoundError: com/nvidia/spark/rapids/GpuColumnVector",
      "  at ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.$anonfun$loadNextBatch$2(GpuPreXGBoost.scala:314)",
      "  at ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$.withResource(GpuPreXGBoost.scala:576)",
      "  at ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.loadNextBatch(GpuPreXGBoost.scala:299)",
      "  at ml.dmlc.xgboost4j.scala.rapids.spark.GpuPreXGBoost$$anon$1.hasNext(GpuPreXGBoost.scala:337)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "  at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)",
      "  at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)",
      "  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)",
      "  at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)",
      "  at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)",
      "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)",
      "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)",
      "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      "Caused by: java.lang.ClassNotFoundException: com.nvidia.spark.rapids.GpuColumnVector",
      "  at java.net.URLClassLoader.findClass(URLClassLoader.java:387)",
      "  at java.lang.ClassLoader.loadClass(ClassLoader.java:418)",
      "  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)",
      "  at java.lang.ClassLoader.loadClass(ClassLoader.java:351)",
      "  ... 47 more",
      ""
     ]
    }
   ],
   "source": [
    "// start transform\n",
    "val (prediction, _) = Benchmark.time(\"transform\") {\n",
    "  val ret = model.transform(transSet).cache()\n",
    "  ret.foreachPartition((_: Iterator[_]) => ())\n",
    "  ret\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "32: error: not found: value prediction",
     "output_type": "error",
     "traceback": [
      "<console>:32: error: not found: value prediction",
      "       prediction.select(\"vendor_id\", \"passenger_count\", \"trip_distance\", labelName, \"prediction\").show(10)",
      "       ^",
      "<console>:35: error: not found: value prediction",
      "         evaluator.evaluate(prediction)",
      "                            ^",
      ""
     ]
    }
   ],
   "source": [
    "prediction.select(\"vendor_id\", \"passenger_count\", \"trip_distance\", labelName, \"prediction\").show(10)\n",
    "val evaluator = new RegressionEvaluator().setLabelCol(labelName)\n",
    "val (rmse, _) = Benchmark.time(\"evaluation\") {\n",
    "  evaluator.evaluate(prediction)\n",
    "}\n",
    "println(s\"RMSE == $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to disk and load model\n",
    "Save the model to disk and then load it to memory. After that use the loaded model to do a new prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite.save(dataRoot + \"/model/taxi\")\n",
    "\n",
    "val modelFromDisk = XGBoostRegressionModel.load(dataRoot + \"/model/taxi\")\n",
    "val (results2, _) = Benchmark.time(\"transform2\") {\n",
    "  modelFromDisk.transform(transSet)\n",
    "}\n",
    "results2.select(\"vendor_id\", \"passenger_count\", \"trip_distance\", labelName, \"prediction\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon_kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
